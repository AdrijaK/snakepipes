import os
import snakePipes.common_functions as cf


### snakemake_workflows initialization ########################################
maindir = os.path.dirname(os.path.dirname(workflow.basedir))

# load conda ENVs (path is relative to "shared/rules" directory)
globals().update(cf.set_env_yamls())

# load config file
globals().update(cf.load_configfile(workflow.overwrite_configfile, config["verbose"]))

# do workflow specific stuff now
include: os.path.join(workflow.basedir, "internals.snakefile")

### include modules of other snakefiles ########################################
################################################################################

# The order of steps is:
# 1. Merge samples across lanes (preprocessing.snakefile)
# 2. Deduplicate (preprocessing.snakefile)
# 3. UMI extraction (umi_tools.snakefile)
# 4. FastQC (FastQC.snakeFile)
# 5. MultiQC

readsUse = reads
if not pairedEnd:
    readsUse = [reads[0]]

deduped = expand("deduplicatedFASTQ/{sample}{RExt}{ext}", sample=samples, RExt=readsUse, ext=ext)
if (sampleSheet and sampleSheet != "") or optDedupDist:
    if sampleSheet and sampleSheet != "":
        sampleDict = readSampleSheet(sampleSheet)
        samples = sampleDict.keys()
        infiles = expand("deduplicatedFASTQ/{sample}{RExt}{ext}", sample=samples.keys(), RExt=readsUse, ext=ext)
    else:
        infiles = expand("deduplicatedFASTQ/{sample}{RExt}{ext}", sample=samples, RExt=readsUse, ext=ext)

    include: os.path.join(maindir, "shared", "rules", "preprocessing.snakefile")

    indir = "deduplicatedFASTQ"
    deduped = infiles

## FASTQ: either downsample FASTQ files or create symlinks to input files
include: os.path.join(maindir, "shared", "rules", "FASTQ.snakefile")

#umi_tools
include: os.path.join(maindir, "shared", "rules", "umi_tools.snakefile")

## FastQC
if fastqc:
    include: os.path.join(maindir, "shared", "rules", "FastQC.snakefile")
    include: os.path.join(maindir, "shared", "rules", "multiQC.snakefile")
else:
    fastqc = False

### conditional/optional rules #################################################
################################################################################
def run_FastQC(fastqc):
    readsUse = reads
    if not pairedEnd:
        readsUse = [reads[0]]
    if fastqc:
        return(["multiQC/multiqc_report.html"] + expand("FastQC/{sample}{read}_fastqc.html", sample = samples, read = readsUse))
    else:
        return([])

# run umi tools
# run fastqc


### execute before  starts #####################################################
################################################################################
onstart:
    if "verbose" in config and config["verbose"]:
        print()
        print("--- Workflow parameters --------------------------------------------------------")
        print("samples:", samples)
        print("paired:", pairedEnd)
        print("read extension:", reads)
        print("fastq dir:", fastq_dir)
        print("Sample sheet:", sampleSheet)
        print("-" * 80, "\n")

        print("--- Environment ----------------------------------------------------------------")
        print("$TMPDIR: ", os.getenv('TMPDIR', ""))
        print("$HOSTNAME: ", os.getenv('HOSTNAME', ""))
        print("-" * 80, "\n")

### main rule ##################################################################
################################################################################

rule all:
    input:
        deduped

### execute after  finished ####################################################
################################################################################
onsuccess:
    cf.cleanLogs(outdir, cluster_config)
    if "verbose" in config and config["verbose"]:
        print("\n--- Preprocessing workflow finished successfully! ------------------------------------\n")
        
onerror:
    print("\n !!! ERROR in preprocessing workflow! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")

